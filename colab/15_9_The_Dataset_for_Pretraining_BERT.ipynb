{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9igX6WvADcWaiWwAamHjO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkim9/Resume/blob/main/15_9_The_Dataset_for_Pretraining_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oWjfJmBq51V",
        "outputId": "acc89d66-c795-48e7-ff6e-0625f9363ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l==1.0.3 in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.4.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l==1.0.3) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->d2l==1.0.3) (2.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.11.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.19.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l==1.0.3) (0.2.8)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.10.6)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.6.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (2.21)\n",
            "Requirement already satisfied: mxnet-cu112==1.9.1 in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112==1.9.1) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112==1.9.1) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112==1.9.1) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.3\n",
        "!pip install -U mxnet-cu112==1.9.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "CSf6x_36q99Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['wikitext-2'] = (\n",
        "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
        "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
        "\n",
        "def _read_wiki(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Uppercase letters are converted to lowercase ones\n",
        "    paragraphs = [line.strip().lower().split(' . ')\n",
        "                  for line in lines if len(line.split(' . ')) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs\n",
        "\n",
        "# nltk _read_wiki\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def _read_wiki_nltk(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Uppercase letters are converted to lowercase ones\n",
        "    paragraphs = [nltk.tokenize.sent_tokenize(line.strip().lower())\n",
        "                  for line in lines if len(nltk.tokenize.sent_tokenize(line.strip())) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs"
      ],
      "metadata": {
        "id": "iekASEGvq_3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b2a585-92b5-4dc0-fc5c-edb58db11fe5"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
        "  # if random.random() <  .5:\n",
        "    if random.random() < 0.5:\n",
        "        is_next = True\n",
        "    else:\n",
        "        # `paragraphs` is a list of lists of lists\n",
        "        next_sentence = random.choice(random.choice(paragraphs))\n",
        "        is_next = False\n",
        "    return sentence, next_sentence, is_next"
      ],
      "metadata": {
        "id": "CRI-J0PxrDbT"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random.choice(random.choice([[1,4,7],[2,5,8],[3,6,9]]))"
      ],
      "metadata": {
        "id": "m9or9Kvw2VLt"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
        "    nsp_data_from_paragraph = []\n",
        "    #             (      7        - 1)\n",
        "    for i in range(len(paragraph) - 1):\n",
        "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
        "            paragraph[i],\n",
        "            paragraph[i + 1],\n",
        "            paragraphs)\n",
        "\n",
        "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
        "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
        "            continue\n",
        "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
        "    return nsp_data_from_paragraph"
      ],
      "metadata": {
        "id": "PK5d2_FZsF-7"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d2l.get_tokens_and_segments(paragraph[0], paragraph[1])"
      ],
      "metadata": {
        "id": "DR4KGh4s7ZfE"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGFbaAH81pWc",
        "outputId": "661360a3-0d38-4c66-a7a8-5c0c56ab6989"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enraged',\n",
              " ',',\n",
              " 'veeru',\n",
              " 'attacks',\n",
              " 'gabbar',\n",
              " \"'s\",\n",
              " 'den',\n",
              " 'and',\n",
              " 'catches',\n",
              " 'the',\n",
              " 'dacoit']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_get_next_sentence(\n",
        "    paragraph[0], paragraph[1], paragraphs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLK_yXcf1iZL",
        "outputId": "62ebe5a0-01d6-4fb2-80f0-4cde0f817eeb"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['veeru', 'returns', ',', 'and', 'jai', 'dies', 'in', 'his', 'arms'],\n",
              " ['the',\n",
              "  'pdr',\n",
              "  'has',\n",
              "  'been',\n",
              "  'constructed',\n",
              "  'in',\n",
              "  'separate',\n",
              "  'link',\n",
              "  'roads',\n",
              "  'of',\n",
              "  'between',\n",
              "  '1',\n",
              "  '@.@',\n",
              "  '61',\n",
              "  'km',\n",
              "  '(',\n",
              "  '1',\n",
              "  '@.@',\n",
              "  '00',\n",
              "  'mi',\n",
              "  ')',\n",
              "  'and',\n",
              "  '5',\n",
              "  '@.@',\n",
              "  '47',\n",
              "  'km',\n",
              "  '(',\n",
              "  '3',\n",
              "  '@.@',\n",
              "  '40',\n",
              "  'mi',\n",
              "  ')',\n",
              "  'around',\n",
              "  'cardiff',\n",
              "  'and',\n",
              "  'to',\n",
              "  'date',\n",
              "  '22',\n",
              "  'kilometres',\n",
              "  '(',\n",
              "  '14',\n",
              "  'mi',\n",
              "  ')',\n",
              "  'including',\n",
              "  'spurs',\n",
              "  'have',\n",
              "  'been',\n",
              "  'opened',\n",
              "  'to',\n",
              "  'traffic',\n",
              "  ',',\n",
              "  'with',\n",
              "  'plans',\n",
              "  'for',\n",
              "  'a',\n",
              "  'further',\n",
              "  '5',\n",
              "  '@.@',\n",
              "  '53',\n",
              "  'km',\n",
              "  '(',\n",
              "  '3',\n",
              "  '@.@',\n",
              "  '44',\n",
              "  'mi',\n",
              "  ')'],\n",
              " False)"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = paragraphs[0]\n",
        "vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "                    '<pad>', '<mask>', '<cls>', '<sep>'])\n"
      ],
      "metadata": {
        "id": "jb6yuPT108Yo"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# range(len(paragraph)-1)"
      ],
      "metadata": {
        "id": "_lkKZE8n1DWy"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _get_nsp_data_from_paragraph(\n",
        "#                 paragraphs[0],\n",
        "#                 paragraphs,\n",
        "#                 d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "#                     '<pad>', '<mask>', '<cls>', '<sep>']),\n",
        "#                 max_len)"
      ],
      "metadata": {
        "id": "_PbDZusO0y0F"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
        "                        vocab):\n",
        "    # For the input of a masked language model, make a new copy of tokens and\n",
        "    # replace some of them by '<mask>' or random tokens\n",
        "    mlm_input_tokens = [token for token in tokens]\n",
        "    pred_positions_and_labels = []\n",
        "    # Shuffle for getting 15% random tokens for prediction in the masked\n",
        "    # language modeling task\n",
        "    random.shuffle(candidate_pred_positions)\n",
        "    for mlm_pred_position in candidate_pred_positions:\n",
        "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
        "            break\n",
        "        masked_token = None\n",
        "        # 80% of the time: replace the word with the '<mask>' token\n",
        "        if random.random() < 0.8:\n",
        "            masked_token = '<mask>'\n",
        "        else:\n",
        "            # 10% of the time: keep the word unchanged\n",
        "            if random.random() < 0.5:\n",
        "                masked_token = tokens[mlm_pred_position]\n",
        "            # 10% of the time: replace the word with a random word\n",
        "            else:\n",
        "                masked_token = random.choice(vocab.idx_to_token)\n",
        "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
        "        pred_positions_and_labels.append(\n",
        "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
        "    return mlm_input_tokens, pred_positions_and_labels"
      ],
      "metadata": {
        "id": "CYUZ7fM_rHP2"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _get_mlm_data_from_tokens  (examples[0][0], vocab)\n",
        "def _get_mlm_data_from_tokens(tokens        , vocab):\n",
        "    candidate_pred_positions = []\n",
        "    # `tokens` is a list of strings\n",
        "    for i, token in enumerate(tokens):\n",
        "        # Special tokens are not predicted in the masked language modeling\n",
        "        # task\n",
        "        if token in ['<cls>', '<sep>']:\n",
        "            continue\n",
        "        candidate_pred_positions.append(i)\n",
        "    # 15% of random tokens are predicted in the masked language modeling task\n",
        "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
        "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
        "                                                    tokens,\n",
        "                                                    candidate_pred_positions,\n",
        "                                                    num_mlm_preds,\n",
        "                                                    vocab)\n",
        "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
        "                                       key=lambda x: x[0])\n",
        "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
        "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
        "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
        "    #          Q                      Qn                  Ans"
      ],
      "metadata": {
        "id": "5zENo1N9rNJT"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _pad_bert_inputs(examples, max_len, vocab):\n",
        "    max_num_mlm_preds = round(max_len * 0.15)\n",
        "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
        "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
        "    nsp_labels = []\n",
        "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
        "         is_next) in examples:\n",
        "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
        "            max_len - len(token_ids)), dtype=torch.long))\n",
        "        all_segments.append(torch.tensor(segments + [0] * (\n",
        "            max_len - len(segments)), dtype=torch.long))\n",
        "        # `valid_lens` excludes count of '<pad>' tokens\n",
        "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
        "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
        "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
        "        # Predictions of padded tokens will be filtered out in the loss via\n",
        "        # multiplication of 0 weights\n",
        "        all_mlm_weights.append(\n",
        "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
        "                max_num_mlm_preds - len(pred_positions)),\n",
        "                dtype=torch.float32))\n",
        "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
        "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
        "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
        "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
        "            all_mlm_weights, all_mlm_labels, nsp_labels)"
      ],
      "metadata": {
        "id": "zq3sH8PerNK_"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class _WikiTextDataset(torch.utils.data.Dataset):\n",
        "#_WikiTextDataset('../data/wikitext-2',  64    )\n",
        "    def __init__ (self, paragraphs    , max_len):\n",
        "        # Input `paragraphs[i]` is a list of sentence strings representing a\n",
        "        # paragraph; while output `paragraphs[i]` is a list of sentences\n",
        "        # representing a paragraph, where each sentence is a list of tokens\n",
        "        paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "        sentences = [sentence        for paragraph in paragraphs\n",
        "                 for sentence         in paragraph]\n",
        "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
        "\n",
        "        # Get data for nsp\n",
        "        examples = []\n",
        "        for paragraph in paragraphs:\n",
        "            examples.extend(_get_nsp_data_from_paragraph(\n",
        "                paragraph, paragraphs, self.vocab, max_len))\n",
        "\n",
        "        # Get data for mlm\n",
        "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
        "                      + (segments, is_next))\n",
        "                     for tokens, segments, is_next in examples]\n",
        "        # Pad inputs\n",
        "        (self.all_token_ids,      self.all_segments, self.valid_lens,\n",
        "         self.all_pred_positions, self.all_mlm_weights,\n",
        "         self.all_mlm_labels,     self.nsp_labels) = _pad_bert_inputs(\n",
        "            examples, max_len, self.vocab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
        "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
        "                self.nsp_labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)"
      ],
      "metadata": {
        "id": "BDO2qLcNrTPV"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examples[0][0]"
      ],
      "metadata": {
        "id": "uwsJjTow_ZPm"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _get_mlm_data_from_tokens(examples[0][0], vocab)"
      ],
      "metadata": {
        "id": "wt85kPdt9ME_"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  examples = []\n",
        "#  examples.extend(_get_nsp_data_from_paragraph(\n",
        "#                 paragraphs[0],\n",
        "#                 paragraphs,\n",
        "#                 d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "#             '<pad>', '<mask>', '<cls>', '<sep>']),\n",
        "#                 max_len))"
      ],
      "metadata": {
        "id": "FDJcf27T0Bjx"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = _read_wiki('../data/wikitext-2')\n",
        "paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "sentences = [sentence        for paragraph in paragraphs\n",
        "                         for sentence in paragraph]"
      ],
      "metadata": {
        "id": "LJLUKTWlvvU2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs_nltk = _read_wiki_nltk('../data/wikitext-2')\n",
        "paragraphs_nltk = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs_nltk]\n",
        "sentences_nltk = [sentence        for paragraph in paragraphs_nltk\n",
        "                 for sentence in paragraph]"
      ],
      "metadata": {
        "id": "o4U18NsGNZeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paragraphs[0]"
      ],
      "metadata": {
        "id": "atkntEGyzTjr"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d2l.Vocab(['Hello world nice to meet you.', 'Always!','www.'], min_freq=1, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])"
      ],
      "metadata": {
        "id": "s-piHqvGyLWg"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences[:3]"
      ],
      "metadata": {
        "id": "YT5TB-j-veys"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.get_dataloader_workers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnBrFdSPsU-B",
        "outputId": "0a6a4f35-e7d8-42f4-cd79-2000c6afbf9e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d2l.download_extract('wikitext-2', 'wikitext-2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "H6eUBkO8smCk",
        "outputId": "ec4fbf1e-9dcd-473a-cdcf-82f10fde5510"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../data/wikitext-2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_data_wiki(batch_size, max_len)\n",
        "load_data_wiki_nltk(batch_size, max_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY1qcuvwsYj5",
        "outputId": "1cbb418c-54c4-4ade-a8b8-33ffda48a39b"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7d1a60998880>,\n",
              " <d2l.torch.Vocab at 0x7d1a6eeecaf0>)"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
        "\n",
        "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
        "     mlm_Y, nsp_y) in train_iter:\n",
        "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
        "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
        "          nsp_y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6OLHi0hrYQr",
        "outputId": "c76d7a07-7d20-4062-8c95-164bc364b18a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, max_len = 512, 64\n",
        "# nltk\n",
        "train_iter_nltk, vocab_nltk = load_data_wiki_nltk(batch_size, max_len)\n",
        "\n",
        "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
        "     mlm_Y, nsp_y) in train_iter_nltk:\n",
        "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
        "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
        "          nsp_y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmGX3IOqKz8Z",
        "outputId": "59aafa9b-8862-4911-a752-fb739bb787c7"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "Pav2lUlgrcJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f01d66a-e169-4581-f153-1c1d01a9d62c"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20256"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGyXpaRhLJ_N",
        "outputId": "61def548-c85c-4e65-9132-534442b2dac4"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20074"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.9.4. Exercises\n",
        "1. Try other sentence splitting techniques\n",
        "using nltk.\n",
        "\n",
        "  \n",
        "  the vocab size decrease from 20256 to 20074\n",
        "\n",
        "2. What is the vocabulary size if we do not filter out any infrequent token?"
      ],
      "metadata": {
        "id": "O8vBbNTzBtNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2CUopXzBhgW",
        "outputId": "921ddd1d-ed59-41de-9f53-6a1bde706ee1"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL9Yjsj2BlbI",
        "outputId": "c52cd1d1-018c-412f-df34-a263116ad08b"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = 'This is great ! Why not ?'\n",
        "nltk.tokenize.sent_tokenize(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk_jug87Bq3v",
        "outputId": "b351acca-1399-4aea-9f74-49114a8689fa"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is great !', 'Why not ?']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['wikitext-2'] = (\n",
        "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
        "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
        "\n",
        "def _read_wiki(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Uppercase letters are converted to lowercase ones\n",
        "    paragraphs = [line.strip().lower().split(' . ')\n",
        "                  for line in lines if len(line.split(' . ')) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs\n",
        "\n",
        "# nltk _read_wiki\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def _read_wiki_nltk(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Uppercase letters are converted to lowercase ones\n",
        "    paragraphs = [nltk.tokenize.sent_tokenize(line.strip().lower())\n",
        "                  for line in lines if len(nltk.tokenize.sent_tokenize(line.strip())) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2enVwu7xCt-f",
        "outputId": "e165eab7-21e0-4d8b-b3c9-8688780561c3"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readWiki = _read_wiki('../data/wikitext-2')\n",
        "# readWiki[:2]"
      ],
      "metadata": {
        "id": "V0i_MqmYINgm"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " nltk_res = _read_wiki_nltk('../data/wikitext-2')\n",
        "#  nltk_res[:2]"
      ],
      "metadata": {
        "id": "aWEd30RyC50k"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_paragraphs = _read_wiki_nltk('../data/wikitext-2')\n",
        "nltk_paragraphs = [d2l.tokenize(\n",
        "            nltk_paragraph, token='word') for nltk_paragraph in nltk_paragraphs]\n",
        "nltk_sentences = [nltk_sentence        for nltk_paragraph in nltk_paragraphs\n",
        "                         for nltk_sentence in nltk_paragraph]"
      ],
      "metadata": {
        "id": "QatEYrNuJucm"
      },
      "execution_count": 262,
      "outputs": []
    }
  ]
}