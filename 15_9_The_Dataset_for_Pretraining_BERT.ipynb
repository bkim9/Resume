{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZbGY9kL3o77To1J4GhMcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkim9/Resume/blob/main/15_9_The_Dataset_for_Pretraining_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oWjfJmBq51V",
        "outputId": "acc89d66-c795-48e7-ff6e-0625f9363ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l==1.0.3 in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from d2l==1.0.3) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.4.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l==1.0.3) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter==1.0.0->d2l==1.0.3) (2.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.11.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.19.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l==1.0.3) (0.2.8)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.10.6)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.6.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (2.21)\n",
            "Requirement already satisfied: mxnet-cu112==1.9.1 in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112==1.9.1) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112==1.9.1) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112==1.9.1) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112==1.9.1) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.3\n",
        "!pip install -U mxnet-cu112==1.9.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "CSf6x_36q99Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['wikitext-2'] = (\n",
        "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
        "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
        "\n",
        "def _read_wiki(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Uppercase letters are converted to lowercase ones\n",
        "    paragraphs = [line.strip().lower().split(' . ')\n",
        "                  for line in lines if len(line.split(' . ')) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs"
      ],
      "metadata": {
        "id": "iekASEGvq_3a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
        "  # if random.random() <  .5:\n",
        "    if random.random() < 0.5:\n",
        "        is_next = True\n",
        "    else:\n",
        "        # `paragraphs` is a list of lists of lists\n",
        "        next_sentence = random.choice(random.choice(paragraphs))\n",
        "        is_next = False\n",
        "    return sentence, next_sentence, is_next"
      ],
      "metadata": {
        "id": "CRI-J0PxrDbT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.choice(random.choice([[1,4,7],[2,5,8],[3,6,9]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9or9Kvw2VLt",
        "outputId": "df7d30a5-9621-4245-a613-32d4ff9555a2"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
        "    nsp_data_from_paragraph = []\n",
        "    #             (      7        - 1)\n",
        "    for i in range(len(paragraph) - 1):\n",
        "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
        "            paragraph[i],\n",
        "            paragraph[i + 1],\n",
        "            paragraphs)\n",
        "\n",
        "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
        "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
        "            continue\n",
        "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
        "    return nsp_data_from_paragraph"
      ],
      "metadata": {
        "id": "PK5d2_FZsF-7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.get_tokens_and_segments(paragraph[0], paragraph[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR4KGh4s7ZfE",
        "outputId": "eef67dee-f24d-493e-e3d7-6b6c498c6312"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<cls>',\n",
              "  'veeru',\n",
              "  'returns',\n",
              "  ',',\n",
              "  'and',\n",
              "  'jai',\n",
              "  'dies',\n",
              "  'in',\n",
              "  'his',\n",
              "  'arms',\n",
              "  '<sep>',\n",
              "  'enraged',\n",
              "  ',',\n",
              "  'veeru',\n",
              "  'attacks',\n",
              "  'gabbar',\n",
              "  \"'s\",\n",
              "  'den',\n",
              "  'and',\n",
              "  'catches',\n",
              "  'the',\n",
              "  'dacoit',\n",
              "  '<sep>'],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGFbaAH81pWc",
        "outputId": "661360a3-0d38-4c66-a7a8-5c0c56ab6989"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enraged',\n",
              " ',',\n",
              " 'veeru',\n",
              " 'attacks',\n",
              " 'gabbar',\n",
              " \"'s\",\n",
              " 'den',\n",
              " 'and',\n",
              " 'catches',\n",
              " 'the',\n",
              " 'dacoit']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_get_next_sentence(\n",
        "    paragraph[0], paragraph[1], paragraphs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLK_yXcf1iZL",
        "outputId": "249e91c9-6e0d-4caa-88fe-29e484ed12b4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['veeru', 'returns', ',', 'and', 'jai', 'dies', 'in', 'his', 'arms'],\n",
              " ['in',\n",
              "  'the',\n",
              "  'corrected',\n",
              "  'timeline',\n",
              "  ',',\n",
              "  'kyle',\n",
              "  ',',\n",
              "  'now',\n",
              "  'raised',\n",
              "  'and',\n",
              "  'trained',\n",
              "  'by',\n",
              "  'both',\n",
              "  'stahn',\n",
              "  'and',\n",
              "  'rutee',\n",
              "  ',',\n",
              "  'goes',\n",
              "  'on',\n",
              "  'a',\n",
              "  'journey',\n",
              "  'to',\n",
              "  'the',\n",
              "  'temple',\n",
              "  'where',\n",
              "  'he',\n",
              "  'first',\n",
              "  'met',\n",
              "  'reala'],\n",
              " False)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = paragraphs[0]\n",
        "vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "                    '<pad>', '<mask>', '<cls>', '<sep>'])\n"
      ],
      "metadata": {
        "id": "jb6yuPT108Yo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range(len(paragraph)-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lkKZE8n1DWy",
        "outputId": "b3b650aa-5ffc-44d8-80cf-c7e66aa50663"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_get_nsp_data_from_paragraph(\n",
        "                paragraphs[0],\n",
        "                paragraphs,\n",
        "                d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "                    '<pad>', '<mask>', '<cls>', '<sep>']),\n",
        "                max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PbDZusO0y0F",
        "outputId": "e910aa19-d2e7-4f20-f811-60195841434a"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['<cls>',\n",
              "   'veeru',\n",
              "   'returns',\n",
              "   ',',\n",
              "   'and',\n",
              "   'jai',\n",
              "   'dies',\n",
              "   'in',\n",
              "   'his',\n",
              "   'arms',\n",
              "   '<sep>',\n",
              "   'enraged',\n",
              "   ',',\n",
              "   'veeru',\n",
              "   'attacks',\n",
              "   'gabbar',\n",
              "   \"'s\",\n",
              "   'den',\n",
              "   'and',\n",
              "   'catches',\n",
              "   'the',\n",
              "   'dacoit',\n",
              "   '<sep>'],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "  True),\n",
              " (['<cls>',\n",
              "   'enraged',\n",
              "   ',',\n",
              "   'veeru',\n",
              "   'attacks',\n",
              "   'gabbar',\n",
              "   \"'s\",\n",
              "   'den',\n",
              "   'and',\n",
              "   'catches',\n",
              "   'the',\n",
              "   'dacoit',\n",
              "   '<sep>',\n",
              "   'the',\n",
              "   'torch',\n",
              "   'of',\n",
              "   'empire',\n",
              "   'then',\n",
              "   'passed',\n",
              "   'from',\n",
              "   'greece',\n",
              "   'to',\n",
              "   'rome',\n",
              "   '.',\n",
              "   '<sep>'],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "  False),\n",
              " (['<cls>',\n",
              "   'veeru',\n",
              "   'nearly',\n",
              "   'beats',\n",
              "   'gabbar',\n",
              "   'to',\n",
              "   'death',\n",
              "   'when',\n",
              "   'thakur',\n",
              "   'appears',\n",
              "   'and',\n",
              "   'reminds',\n",
              "   'veeru',\n",
              "   'of',\n",
              "   'the',\n",
              "   'promise',\n",
              "   'to',\n",
              "   'hand',\n",
              "   'over',\n",
              "   'gabbar',\n",
              "   'alive',\n",
              "   '<sep>',\n",
              "   'he',\n",
              "   'competed',\n",
              "   'professionally',\n",
              "   'for',\n",
              "   'yeovil',\n",
              "   'town',\n",
              "   ',',\n",
              "   'hereford',\n",
              "   'united',\n",
              "   'and',\n",
              "   'exeter',\n",
              "   'city',\n",
              "   ',',\n",
              "   'and',\n",
              "   'won',\n",
              "   'promotion',\n",
              "   'from',\n",
              "   'the',\n",
              "   'football',\n",
              "   'conference',\n",
              "   'to',\n",
              "   'the',\n",
              "   'football',\n",
              "   'league',\n",
              "   'with',\n",
              "   'all',\n",
              "   'three',\n",
              "   'teams',\n",
              "   '.',\n",
              "   '<sep>'],\n",
              "  [0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1],\n",
              "  False),\n",
              " (['<cls>',\n",
              "   'thakur',\n",
              "   'uses',\n",
              "   'his',\n",
              "   'spike',\n",
              "   '@-@',\n",
              "   '<unk>',\n",
              "   'shoes',\n",
              "   'to',\n",
              "   'severely',\n",
              "   '<unk>',\n",
              "   'gabbar',\n",
              "   'and',\n",
              "   'destroy',\n",
              "   'his',\n",
              "   'hands',\n",
              "   '<sep>',\n",
              "   'the',\n",
              "   'police',\n",
              "   'then',\n",
              "   'arrive',\n",
              "   'and',\n",
              "   'arrest',\n",
              "   'gabbar',\n",
              "   '<sep>'],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "  True),\n",
              " (['<cls>',\n",
              "   'the',\n",
              "   'police',\n",
              "   'then',\n",
              "   'arrive',\n",
              "   'and',\n",
              "   'arrest',\n",
              "   'gabbar',\n",
              "   '<sep>',\n",
              "   'after',\n",
              "   'jai',\n",
              "   \"'s\",\n",
              "   'funeral',\n",
              "   ',',\n",
              "   'veeru',\n",
              "   'leaves',\n",
              "   'ramgarh',\n",
              "   'and',\n",
              "   'finds',\n",
              "   'basanti',\n",
              "   'waiting',\n",
              "   'for',\n",
              "   'him',\n",
              "   'on',\n",
              "   'the',\n",
              "   'train',\n",
              "   '<sep>'],\n",
              "  [0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1],\n",
              "  True),\n",
              " (['<cls>',\n",
              "   'after',\n",
              "   'jai',\n",
              "   \"'s\",\n",
              "   'funeral',\n",
              "   ',',\n",
              "   'veeru',\n",
              "   'leaves',\n",
              "   'ramgarh',\n",
              "   'and',\n",
              "   'finds',\n",
              "   'basanti',\n",
              "   'waiting',\n",
              "   'for',\n",
              "   'him',\n",
              "   'on',\n",
              "   'the',\n",
              "   'train',\n",
              "   '<sep>',\n",
              "   'radha',\n",
              "   'is',\n",
              "   'left',\n",
              "   'alone',\n",
              "   'again',\n",
              "   '.',\n",
              "   '<sep>'],\n",
              "  [0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1],\n",
              "  True)]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
        "                        vocab):\n",
        "    # For the input of a masked language model, make a new copy of tokens and\n",
        "    # replace some of them by '<mask>' or random tokens\n",
        "    mlm_input_tokens = [token for token in tokens]\n",
        "    pred_positions_and_labels = []\n",
        "    # Shuffle for getting 15% random tokens for prediction in the masked\n",
        "    # language modeling task\n",
        "    random.shuffle(candidate_pred_positions)\n",
        "    for mlm_pred_position in candidate_pred_positions:\n",
        "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
        "            break\n",
        "        masked_token = None\n",
        "        # 80% of the time: replace the word with the '<mask>' token\n",
        "        if random.random() < 0.8:\n",
        "            masked_token = '<mask>'\n",
        "        else:\n",
        "            # 10% of the time: keep the word unchanged\n",
        "            if random.random() < 0.5:\n",
        "                masked_token = tokens[mlm_pred_position]\n",
        "            # 10% of the time: replace the word with a random word\n",
        "            else:\n",
        "                masked_token = random.choice(vocab.idx_to_token)\n",
        "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
        "        pred_positions_and_labels.append(\n",
        "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
        "    return mlm_input_tokens, pred_positions_and_labels"
      ],
      "metadata": {
        "id": "CYUZ7fM_rHP2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _get_mlm_data_from_tokens  (examples[0][0], vocab)\n",
        "def _get_mlm_data_from_tokens(tokens        , vocab):\n",
        "    candidate_pred_positions = []\n",
        "    # `tokens` is a list of strings\n",
        "    for i, token in enumerate(tokens):\n",
        "        # Special tokens are not predicted in the masked language modeling\n",
        "        # task\n",
        "        if token in ['<cls>', '<sep>']:\n",
        "            continue\n",
        "        candidate_pred_positions.append(i)\n",
        "    # 15% of random tokens are predicted in the masked language modeling task\n",
        "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
        "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
        "                                                    tokens,\n",
        "                                                    candidate_pred_positions,\n",
        "                                                    num_mlm_preds,\n",
        "                                                    vocab)\n",
        "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
        "                                       key=lambda x: x[0])\n",
        "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
        "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
        "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
        "    #          Q                      Qn                  Ans"
      ],
      "metadata": {
        "id": "5zENo1N9rNJT"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _pad_bert_inputs(examples, max_len, vocab):\n",
        "    max_num_mlm_preds = round(max_len * 0.15)\n",
        "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
        "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
        "    nsp_labels = []\n",
        "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
        "         is_next) in examples:\n",
        "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
        "            max_len - len(token_ids)), dtype=torch.long))\n",
        "        all_segments.append(torch.tensor(segments + [0] * (\n",
        "            max_len - len(segments)), dtype=torch.long))\n",
        "        # `valid_lens` excludes count of '<pad>' tokens\n",
        "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
        "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
        "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
        "        # Predictions of padded tokens will be filtered out in the loss via\n",
        "        # multiplication of 0 weights\n",
        "        all_mlm_weights.append(\n",
        "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
        "                max_num_mlm_preds - len(pred_positions)),\n",
        "                dtype=torch.float32))\n",
        "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
        "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
        "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
        "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
        "            all_mlm_weights, all_mlm_labels, nsp_labels)"
      ],
      "metadata": {
        "id": "zq3sH8PerNK_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class _WikiTextDataset(torch.utils.data.Dataset):\n",
        "#_WikiTextDataset('../data/wikitext-2',  64    )\n",
        "    def __init__ (self, paragraphs    , max_len):\n",
        "        # Input `paragraphs[i]` is a list of sentence strings representing a\n",
        "        # paragraph; while output `paragraphs[i]` is a list of sentences\n",
        "        # representing a paragraph, where each sentence is a list of tokens\n",
        "        paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "        sentences = [sentence        for paragraph in paragraphs\n",
        "                 for sentence         in paragraph]\n",
        "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
        "\n",
        "        # Get data for nsp\n",
        "        examples = []\n",
        "        for paragraph in paragraphs:\n",
        "            examples.extend(_get_nsp_data_from_paragraph(\n",
        "                paragraph, paragraphs, self.vocab, max_len))\n",
        "\n",
        "        # Get data for mlm\n",
        "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
        "                      + (segments, is_next))\n",
        "                     for tokens, segments, is_next in examples]\n",
        "        # Pad inputs\n",
        "        (self.all_token_ids,      self.all_segments, self.valid_lens,\n",
        "         self.all_pred_positions, self.all_mlm_weights,\n",
        "         self.all_mlm_labels,     self.nsp_labels) = _pad_bert_inputs(\n",
        "            examples, max_len, self.vocab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
        "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
        "                self.nsp_labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)"
      ],
      "metadata": {
        "id": "BDO2qLcNrTPV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwsJjTow_ZPm",
        "outputId": "d3c91abe-081d-4aa3-cd81-70326d70d78e"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<cls>',\n",
              " 'veeru',\n",
              " 'returns',\n",
              " ',',\n",
              " 'and',\n",
              " 'jai',\n",
              " 'dies',\n",
              " 'in',\n",
              " 'his',\n",
              " 'arms',\n",
              " '<sep>',\n",
              " 'he',\n",
              " 'improvised',\n",
              " 'the',\n",
              " 'sermon',\n",
              " ';',\n",
              " 'the',\n",
              " 'producer',\n",
              " ',',\n",
              " 'david',\n",
              " 'o',\n",
              " '<unk>',\n",
              " ',',\n",
              " '<unk>',\n",
              " 'called',\n",
              " 'for',\n",
              " 'retake',\n",
              " 'after',\n",
              " 'retake',\n",
              " 'to',\n",
              " 'try',\n",
              " 'to',\n",
              " 'make',\n",
              " 'him',\n",
              " 'dry',\n",
              " 'up',\n",
              " ',',\n",
              " 'but',\n",
              " 'walpole',\n",
              " '<unk>',\n",
              " 'delivered',\n",
              " 'a',\n",
              " 'different',\n",
              " '<unk>',\n",
              " 'address',\n",
              " 'each',\n",
              " 'time',\n",
              " '.',\n",
              " '<sep>']"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_get_mlm_data_from_tokens(examples[0][0], vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt85kPdt9ME_",
        "outputId": "0d072d37-62c5-45b1-b32e-6cdc4527ea8b"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([942,\n",
              "  19302,\n",
              "  15626,\n",
              "  49,\n",
              "  1622,\n",
              "  10123,\n",
              "  5758,\n",
              "  9515,\n",
              "  9052,\n",
              "  1927,\n",
              "  945,\n",
              "  8838,\n",
              "  9513,\n",
              "  18282,\n",
              "  16496,\n",
              "  940,\n",
              "  18282,\n",
              "  14485,\n",
              "  49,\n",
              "  5302,\n",
              "  12910,\n",
              "  946,\n",
              "  49,\n",
              "  946,\n",
              "  943,\n",
              "  7743,\n",
              "  15609,\n",
              "  1299,\n",
              "  943,\n",
              "  18468,\n",
              "  18828,\n",
              "  18468,\n",
              "  11408,\n",
              "  9033,\n",
              "  6258,\n",
              "  943,\n",
              "  943,\n",
              "  3278,\n",
              "  943,\n",
              "  946,\n",
              "  5476,\n",
              "  943,\n",
              "  5765,\n",
              "  946,\n",
              "  943,\n",
              "  6364,\n",
              "  18435,\n",
              "  51,\n",
              "  945],\n",
              " [24, 28, 35, 36, 38, 41, 44],\n",
              " [3357, 15609, 19143, 49, 19594, 956, 1164])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " examples = []\n",
        " examples.extend(_get_nsp_data_from_paragraph(\n",
        "                paragraphs[0],\n",
        "                paragraphs,\n",
        "                d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>']),\n",
        "                max_len))"
      ],
      "metadata": {
        "id": "FDJcf27T0Bjx"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = _read_wiki('../data/wikitext-2')\n",
        "paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "sentences = [sentence        for paragraph in paragraphs\n",
        "                         for sentence in paragraph]"
      ],
      "metadata": {
        "id": "LJLUKTWlvvU2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atkntEGyzTjr",
        "outputId": "9309914e-6eec-4a94-ddbc-f97d0748b5c3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['veeru', 'returns', ',', 'and', 'jai', 'dies', 'in', 'his', 'arms'],\n",
              "  ['enraged',\n",
              "   ',',\n",
              "   'veeru',\n",
              "   'attacks',\n",
              "   'gabbar',\n",
              "   \"'s\",\n",
              "   'den',\n",
              "   'and',\n",
              "   'catches',\n",
              "   'the',\n",
              "   'dacoit'],\n",
              "  ['veeru',\n",
              "   'nearly',\n",
              "   'beats',\n",
              "   'gabbar',\n",
              "   'to',\n",
              "   'death',\n",
              "   'when',\n",
              "   'thakur',\n",
              "   'appears',\n",
              "   'and',\n",
              "   'reminds',\n",
              "   'veeru',\n",
              "   'of',\n",
              "   'the',\n",
              "   'promise',\n",
              "   'to',\n",
              "   'hand',\n",
              "   'over',\n",
              "   'gabbar',\n",
              "   'alive'],\n",
              "  ['thakur',\n",
              "   'uses',\n",
              "   'his',\n",
              "   'spike',\n",
              "   '@-@',\n",
              "   '<unk>',\n",
              "   'shoes',\n",
              "   'to',\n",
              "   'severely',\n",
              "   '<unk>',\n",
              "   'gabbar',\n",
              "   'and',\n",
              "   'destroy',\n",
              "   'his',\n",
              "   'hands'],\n",
              "  ['the', 'police', 'then', 'arrive', 'and', 'arrest', 'gabbar'],\n",
              "  ['after',\n",
              "   'jai',\n",
              "   \"'s\",\n",
              "   'funeral',\n",
              "   ',',\n",
              "   'veeru',\n",
              "   'leaves',\n",
              "   'ramgarh',\n",
              "   'and',\n",
              "   'finds',\n",
              "   'basanti',\n",
              "   'waiting',\n",
              "   'for',\n",
              "   'him',\n",
              "   'on',\n",
              "   'the',\n",
              "   'train'],\n",
              "  ['radha', 'is', 'left', 'alone', 'again', '.']],\n",
              " [['common',\n",
              "   'starlings',\n",
              "   'may',\n",
              "   'be',\n",
              "   'kept',\n",
              "   'as',\n",
              "   'pets',\n",
              "   'or',\n",
              "   'as',\n",
              "   'laboratory',\n",
              "   'animals'],\n",
              "  ['austrian',\n",
              "   '<unk>',\n",
              "   'konrad',\n",
              "   'lorenz',\n",
              "   'wrote',\n",
              "   'of',\n",
              "   'them',\n",
              "   'in',\n",
              "   'his',\n",
              "   'book',\n",
              "   'king',\n",
              "   'solomon',\n",
              "   \"'s\",\n",
              "   'ring',\n",
              "   'as',\n",
              "   '\"',\n",
              "   'the',\n",
              "   'poor',\n",
              "   'man',\n",
              "   \"'s\",\n",
              "   'dog',\n",
              "   '\"',\n",
              "   'and',\n",
              "   '\"',\n",
              "   'something',\n",
              "   'to',\n",
              "   'love',\n",
              "   '\"',\n",
              "   ',',\n",
              "   'because',\n",
              "   'nestlings',\n",
              "   'are',\n",
              "   'easily',\n",
              "   'obtained',\n",
              "   'from',\n",
              "   'the',\n",
              "   'wild',\n",
              "   'and',\n",
              "   'after',\n",
              "   'careful',\n",
              "   'hand',\n",
              "   'rearing',\n",
              "   'they',\n",
              "   'are',\n",
              "   'straightforward',\n",
              "   'to',\n",
              "   'look',\n",
              "   'after'],\n",
              "  ['they',\n",
              "   'adapt',\n",
              "   'well',\n",
              "   'to',\n",
              "   'captivity',\n",
              "   ',',\n",
              "   'and',\n",
              "   'thrive',\n",
              "   'on',\n",
              "   'a',\n",
              "   'diet',\n",
              "   'of',\n",
              "   'standard',\n",
              "   'bird',\n",
              "   'feed',\n",
              "   'and',\n",
              "   '<unk>'],\n",
              "  ['several',\n",
              "   'birds',\n",
              "   'may',\n",
              "   'be',\n",
              "   'kept',\n",
              "   'in',\n",
              "   'the',\n",
              "   'same',\n",
              "   'cage',\n",
              "   ',',\n",
              "   'and',\n",
              "   'their',\n",
              "   '<unk>',\n",
              "   'makes',\n",
              "   'them',\n",
              "   'easy',\n",
              "   'to',\n",
              "   'train',\n",
              "   'or',\n",
              "   'study'],\n",
              "  ['the',\n",
              "   'only',\n",
              "   'disadvantages',\n",
              "   'are',\n",
              "   'their',\n",
              "   '<unk>',\n",
              "   'and',\n",
              "   'indiscriminate',\n",
              "   'defecation',\n",
              "   'habits',\n",
              "   'and',\n",
              "   'the',\n",
              "   'need',\n",
              "   'to',\n",
              "   'take',\n",
              "   'precautions',\n",
              "   'against',\n",
              "   'diseases',\n",
              "   'that',\n",
              "   'may',\n",
              "   'be',\n",
              "   'transmitted',\n",
              "   'to',\n",
              "   'humans'],\n",
              "  ['as',\n",
              "   'a',\n",
              "   'laboratory',\n",
              "   'bird',\n",
              "   ',',\n",
              "   'the',\n",
              "   'common',\n",
              "   'starling',\n",
              "   'is',\n",
              "   'second',\n",
              "   'in',\n",
              "   'numbers',\n",
              "   'only',\n",
              "   'to',\n",
              "   'the',\n",
              "   'domestic',\n",
              "   '<unk>',\n",
              "   '.']]]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.Vocab(['Hello world nice to meet you.', 'Always!','www.'], min_freq=1, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-piHqvGyLWg",
        "outputId": "6d8b2766-78c1-49eb-bfca-1f9fe64d5fb4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<d2l.torch.Vocab at 0x7d1a6ef5b400>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT5TB-j-veys",
        "outputId": "7f450a9d-40db-4bf8-cee1-3ded176541c6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the dangerously in love tour was the debut concert tour by american recording artist beyonc',\n",
              " \"although the tour was intended to showcase songs from her debut solo album , dangerously in love , ( 2003 ) the set list also contained a special segment dedicated to beyonc 's girl group destiny 's child and featured songs from her 2003 film the fighting temptations\",\n",
              " 'the stage was simple and featured a large led screen in the back that displayed video images of beyonc and her dancers , as well as some images from her music videos and some <unk> images']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_wiki(batch_size, max_len):\n",
        "    \"\"\"Load the WikiText-2 dataset.\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
        "    # data_dir: '../data/wikitext-2'\n",
        "    paragraphs = _read_wiki(data_dir)\n",
        "    #           _WikiTextDataset(_read_wiki('../data/wikitext-2'),  64    )\n",
        "    train_set = _WikiTextDataset(paragraphs,           max_len)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
        "                                        shuffle=True, num_workers=num_workers)\n",
        "    return train_iter, train_set.vocab"
      ],
      "metadata": {
        "id": "xtLkviqbrV1O"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.get_dataloader_workers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnBrFdSPsU-B",
        "outputId": "0a6a4f35-e7d8-42f4-cd79-2000c6afbf9e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.download_extract('wikitext-2', 'wikitext-2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "H6eUBkO8smCk",
        "outputId": "ec4fbf1e-9dcd-473a-cdcf-82f10fde5510"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../data/wikitext-2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_data_wiki(batch_size, max_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY1qcuvwsYj5",
        "outputId": "af074507-9311-4c5b-c306-7e5fa2b24d40"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7d1b5f32b430>,\n",
              " <d2l.torch.Vocab at 0x7d1a67379f00>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
        "\n",
        "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
        "     mlm_Y, nsp_y) in train_iter:\n",
        "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
        "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
        "          nsp_y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6OLHi0hrYQr",
        "outputId": "c76d7a07-7d20-4062-8c95-164bc364b18a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "Pav2lUlgrcJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f2e747-7f9b-4d7b-db55-00f283ae4287"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20256"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}